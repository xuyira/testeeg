#!/usr/bin/env python3
"""
ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Œåˆ¤æ–­æ˜¯å¦æ”¶æ•›
ä½¿ç”¨æ–¹æ³•ï¼špython scripts/monitor_training.py --log_dir logs/
"""

import argparse
import re
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path


def parse_log_file(log_file):
    """è§£æè®­ç»ƒæ—¥å¿—æ–‡ä»¶"""
    steps = []
    losses = []
    loss_q0s = []
    grad_norms = []
    
    with open(log_file, 'r') as f:
        content = f.read()
        
    # åŒ¹é…æ‰€æœ‰çš„stepå’Œlossè®°å½•
    step_pattern = r'\| step\s+\| ([\d.e\+\-]+)'
    loss_pattern = r'\| loss\s+\| ([\d.e\+\-]+)'
    loss_q0_pattern = r'\| loss_q0\s+\| ([\d.e\+\-]+)'
    grad_norm_pattern = r'\| grad_norm\s+\| ([\d.e\+\-]+)'
    
    step_matches = re.findall(step_pattern, content)
    loss_matches = re.findall(loss_pattern, content)
    loss_q0_matches = re.findall(loss_q0_pattern, content)
    grad_norm_matches = re.findall(grad_norm_pattern, content)
    
    for i in range(min(len(step_matches), len(loss_matches))):
        try:
            steps.append(float(step_matches[i]))
            losses.append(float(loss_matches[i]))
            if i < len(loss_q0_matches):
                loss_q0s.append(float(loss_q0_matches[i]))
            if i < len(grad_norm_matches):
                grad_norms.append(float(grad_norm_matches[i]))
        except:
            continue
    
    return np.array(steps), np.array(losses), np.array(loss_q0s), np.array(grad_norms)


def check_convergence(losses, window=50, threshold=0.01):
    """
    æ£€æŸ¥æ˜¯å¦æ”¶æ•›
    :param losses: lossæ•°ç»„
    :param window: è§‚å¯Ÿçª—å£å¤§å°
    :param threshold: ç›¸å¯¹å˜åŒ–é˜ˆå€¼ï¼ˆ1%ï¼‰
    """
    if len(losses) < window + 10:
        return False, "æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•åˆ¤æ–­"
    
    # è®¡ç®—æœ€è¿‘windowæ­¥çš„losså‡å€¼
    recent_mean = np.mean(losses[-window:])
    # è®¡ç®—ä¹‹å‰windowæ­¥çš„losså‡å€¼
    previous_mean = np.mean(losses[-2*window:-window])
    
    # è®¡ç®—ç›¸å¯¹å˜åŒ–
    if previous_mean > 0:
        relative_change = abs(recent_mean - previous_mean) / previous_mean
        
        if relative_change < threshold:
            return True, f"æ”¶æ•›ï¼æœ€è¿‘{window}æ­¥losså˜åŒ– < {threshold*100}%"
        else:
            return False, f"ä»åœ¨è®­ç»ƒä¸­ï¼Œæœ€è¿‘{window}æ­¥losså˜åŒ–: {relative_change*100:.2f}%"
    
    return False, "æ— æ³•è®¡ç®—"


def plot_training_curves(steps, losses, loss_q0s, grad_norms, save_path=None):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. Lossæ›²çº¿
    axes[0, 0].plot(steps, losses, 'b-', alpha=0.3, label='Raw Loss')
    if len(losses) > 100:
        # å¹³æ»‘æ›²çº¿
        window = min(100, len(losses) // 10)
        smooth_loss = np.convolve(losses, np.ones(window)/window, mode='valid')
        smooth_steps = steps[:len(smooth_loss)]
        axes[0, 0].plot(smooth_steps, smooth_loss, 'r-', linewidth=2, label=f'Smoothed (window={window})')
    axes[0, 0].set_xlabel('Step')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_yscale('log')
    
    # 2. Loss_q0æ›²çº¿
    if len(loss_q0s) > 0:
        axes[0, 1].plot(steps[:len(loss_q0s)], loss_q0s, 'g-', alpha=0.3)
        if len(loss_q0s) > 100:
            window = min(100, len(loss_q0s) // 10)
            smooth_q0 = np.convolve(loss_q0s, np.ones(window)/window, mode='valid')
            smooth_steps = steps[:len(smooth_q0)]
            axes[0, 1].plot(smooth_steps, smooth_q0, 'darkgreen', linewidth=2)
        axes[0, 1].set_xlabel('Step')
        axes[0, 1].set_ylabel('Loss Q0')
        axes[0, 1].set_title('Loss Q0 (Hardest Timesteps)')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_yscale('log')
    
    # 3. æ¢¯åº¦èŒƒæ•°
    if len(grad_norms) > 0:
        axes[1, 0].plot(steps[:len(grad_norms)], grad_norms, 'm-', alpha=0.5)
        axes[1, 0].set_xlabel('Step')
        axes[1, 0].set_ylabel('Gradient Norm')
        axes[1, 0].set_title('Gradient Norm')
        axes[1, 0].grid(True, alpha=0.3)
    
    # 4. æœ€è¿‘çš„Lossè¯¦æƒ…
    if len(losses) > 1000:
        recent_steps = steps[-1000:]
        recent_losses = losses[-1000:]
        axes[1, 1].plot(recent_steps, recent_losses, 'b-', linewidth=2)
        axes[1, 1].set_xlabel('Step')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].set_title('Recent 1000 Steps - Loss Detail')
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
    else:
        plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')
        print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: training_curves.png")
    
    plt.close()


def main():
    parser = argparse.ArgumentParser(description='ç›‘æ§æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿›åº¦')
    parser.add_argument('--log_dir', type=str, default='logs/', help='æ—¥å¿—ç›®å½•è·¯å¾„')
    parser.add_argument('--log_file', type=str, default=None, help='å…·ä½“æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--window', type=int, default=50, help='æ”¶æ•›æ£€æŸ¥çª—å£å¤§å°')
    parser.add_argument('--threshold', type=float, default=0.01, help='æ”¶æ•›é˜ˆå€¼ï¼ˆé»˜è®¤1%ï¼‰')
    args = parser.parse_args()
    
    # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
    if args.log_file:
        log_file = Path(args.log_file)
    else:
        log_dir = Path(args.log_dir)
        # å°è¯•æ‰¾åˆ°æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
        log_files = list(log_dir.glob('*.txt')) + list(log_dir.glob('*.log'))
        if not log_files:
            print(f"é”™è¯¯ï¼šåœ¨ {log_dir} ä¸­æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            print("è¯·æ‰‹åŠ¨æŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼š--log_file path/to/logfile")
            return
        log_file = sorted(log_files, key=lambda x: x.stat().st_mtime)[-1]
    
    if not log_file.exists():
        print(f"é”™è¯¯ï¼šæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
        return
    
    print(f"è¯»å–æ—¥å¿—æ–‡ä»¶: {log_file}")
    print("=" * 60)
    
    try:
        steps, losses, loss_q0s, grad_norms = parse_log_file(log_file)
        
        if len(losses) == 0:
            print("è­¦å‘Šï¼šæœªèƒ½ä»æ—¥å¿—æ–‡ä»¶ä¸­è§£æåˆ°lossæ•°æ®")
            return
        
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"  æ€»æ­¥æ•°: {int(steps[-1])}")
        print(f"  è®°å½•ç‚¹æ•°: {len(losses)}")
        print(f"  å½“å‰Loss: {losses[-1]:.6f}")
        print(f"  åˆå§‹Loss: {losses[0]:.6f}")
        print(f"  Lossä¸‹é™: {(1 - losses[-1]/losses[0])*100:.2f}%")
        
        if len(grad_norms) > 0:
            print(f"  å½“å‰æ¢¯åº¦èŒƒæ•°: {grad_norms[-1]:.6f}")
        
        print("\n" + "=" * 60)
        
        # æ£€æŸ¥æ”¶æ•›
        converged, message = check_convergence(losses, window=args.window, threshold=args.threshold)
        
        print(f"\nğŸ” æ”¶æ•›åˆ†æ (è§‚å¯Ÿçª—å£={args.window}æ­¥):")
        print(f"  {message}")
        
        if converged:
            print("\nâœ… å»ºè®®ï¼šæ¨¡å‹å¯èƒ½å·²æ”¶æ•›ï¼Œå¯ä»¥è€ƒè™‘åœæ­¢è®­ç»ƒ")
        else:
            # ä¼°ç®—è¿˜éœ€è¦å¤šä¹…
            if len(losses) > 100:
                recent_rate = (losses[-100] - losses[-1]) / 100
                if recent_rate > 0:
                    print(f"\nâ³ Lossä»åœ¨ä¸‹é™ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒ")
                    print(f"  è¿‘100æ­¥å¹³å‡ä¸‹é™é€Ÿç‡: {recent_rate:.8f}/step")
        
        print("\n" + "=" * 60)
        print("\nğŸ“ˆ ç”Ÿæˆè®­ç»ƒæ›²çº¿...")
        
        # ç»˜åˆ¶æ›²çº¿
        save_path = log_file.parent / "training_curves.png"
        plot_training_curves(steps, losses, loss_q0s, grad_norms, save_path)
        
        print("\nâœ¨ åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"é”™è¯¯ï¼šå¤„ç†æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

